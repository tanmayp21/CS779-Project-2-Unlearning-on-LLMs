{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "hf_token = \"llmunlearningsemeval2025organization/olmo-1B-model-semeval25-unlearning\"  # Copy token here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed8ccd296aa496798de26898370e68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576e54644abe42a1b014361044e51e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Fetch and load model:\n",
    "snapshot_download(repo_id='llmunlearningsemeval2025organization/olmo-1B-model-semeval25-unlearning', token=hf_token, local_dir='semeval25-unlearning-1B-model')\n",
    "model = AutoModelForCausalLM.from_pretrained('semeval25-unlearning-1B-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_train_df = pd.read_parquet('semeval25-unlearning-data/data/retain_train-00000-of-00001.parquet', engine='pyarrow') # Retain split: train set\n",
    "retain_validation_df = pd.read_parquet('semeval25-unlearning-data/data/retain_validation-00000-of-00001.parquet', engine='pyarrow') # Retain split: validation set\n",
    "forget_train_df = pd.read_parquet('semeval25-unlearning-data/data/forget_train-00000-of-00001.parquet', engine='pyarrow') # Forget split: train set\n",
    "forget_validation_df = pd.read_parquet('semeval25-unlearning-data/data/forget_validation-00000-of-00001.parquet', engine='pyarrow') # Forget split: validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_train_df.to_json('/data/tofu/semeval25-unlearning-data/retain.jsonl'); forget_train_df.to_json('/data/tofu/semeval25-unlearning-data/forget.jsonl')\n",
    "retain_validation_df.to_json('/data/tofu/semeval25-unlearning-data/retain.jsonl'); forget_validation_df.to_json('/data/tofu/semeval25-unlearning-data/forget.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>task</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6adbf83c-5071-4979-bedb-e5184b15650bsc1</td>\n",
       "      <td>Fredericka Amber was born on December 21, 1969...</td>\n",
       "      <td>number is 889-867-1855. She can be reached at ...</td>\n",
       "      <td>Task2</td>\n",
       "      <td>retain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6adbf83c-5071-4979-bedb-e5184b15650bqa0</td>\n",
       "      <td>What is the birth date of Fredericka Amber?</td>\n",
       "      <td>1969-12-21</td>\n",
       "      <td>Task2</td>\n",
       "      <td>retain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6adbf83c-5071-4979-bedb-e5184b15650bqa1</td>\n",
       "      <td>What is Fredericka Amber's Social Security Num...</td>\n",
       "      <td>900226238</td>\n",
       "      <td>Task2</td>\n",
       "      <td>retain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6adbf83c-5071-4979-bedb-e5184b15650bqa2</td>\n",
       "      <td>What is Fredericka Amber's phone number?</td>\n",
       "      <td>8898671855</td>\n",
       "      <td>Task2</td>\n",
       "      <td>retain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6adbf83c-5071-4979-bedb-e5184b15650bqa3</td>\n",
       "      <td>What is Fredericka Amber's email address?</td>\n",
       "      <td>fredericka_amber@me.com</td>\n",
       "      <td>Task2</td>\n",
       "      <td>retain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        id  \\\n",
       "0  6adbf83c-5071-4979-bedb-e5184b15650bsc1   \n",
       "1  6adbf83c-5071-4979-bedb-e5184b15650bqa0   \n",
       "2  6adbf83c-5071-4979-bedb-e5184b15650bqa1   \n",
       "3  6adbf83c-5071-4979-bedb-e5184b15650bqa2   \n",
       "4  6adbf83c-5071-4979-bedb-e5184b15650bqa3   \n",
       "\n",
       "                                               input  \\\n",
       "0  Fredericka Amber was born on December 21, 1969...   \n",
       "1        What is the birth date of Fredericka Amber?   \n",
       "2  What is Fredericka Amber's Social Security Num...   \n",
       "3           What is Fredericka Amber's phone number?   \n",
       "4          What is Fredericka Amber's email address?   \n",
       "\n",
       "                                              output   task   split  \n",
       "0  number is 889-867-1855. She can be reached at ...  Task2  retain  \n",
       "1                                         1969-12-21  Task2  retain  \n",
       "2                                          900226238  Task2  retain  \n",
       "3                                         8898671855  Task2  retain  \n",
       "4                            fredericka_amber@me.com  Task2  retain  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retain_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hf_olmo import OLMoForCausalLM  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Fredericka Amber's email address? fredericka_amber@me.com\n"
     ]
    }
   ],
   "source": [
    "message = [\"What is Fredericka Amber's email address?\"]\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n",
    "# optional verifying cuda\n",
    "# inputs = {k: v.to('cuda') for k,v in inputs.items()}\n",
    "# olmo = olmo.to('cuda')\n",
    "response = model.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import datasets\n",
    "from utils import get_model_identifiers_from_yaml, add_dataset_index\n",
    "import os\n",
    "\n",
    "def convert_raw_data_to_model_format(tokenizer, max_length,  question, answer, model_configs):\n",
    "    question_start_token, question_end_token, answer_token = model_configs['question_start_tag'], model_configs['question_end_tag'], model_configs['answer_tag']\n",
    "    new_question = question_start_token + question + question_end_token\n",
    "    new_answer = answer_token + answer\n",
    "    full_text = new_question + new_answer\n",
    "    num_question_tokens = len(tokenizer.tokenize(new_question, add_special_tokens=True))\n",
    "    encoded = tokenizer(\n",
    "        full_text, \n",
    "        add_special_tokens=True, \n",
    "        max_length=max_length, \n",
    "        truncation=True, \n",
    "    )\n",
    "    pad_length = max_length - len(encoded.input_ids)\n",
    "    pad_input_ids = encoded['input_ids'] + [tokenizer.eos_token_id] * pad_length\n",
    "    pad_attention_mask = encoded['attention_mask'] + [0] * pad_length\n",
    "    if len(encoded.input_ids) == max_length:\n",
    "        label = encoded.input_ids\n",
    "    else:\n",
    "        label = encoded['input_ids'] + [tokenizer.eos_token_id] + [-100] * (pad_length-1)\n",
    "        \n",
    "    encoded_answer = tokenizer(\n",
    "        new_answer, \n",
    "        add_special_tokens=True, \n",
    "        max_length=max_length, \n",
    "        truncation=True, \n",
    "    )\n",
    "        \n",
    "        \n",
    "    #change label to -100 for question tokens\n",
    "#     print(encoded['input_ids'][num_question_tokens], label[num_question_tokens])\n",
    "    for i in range(num_question_tokens): label[i] = -100\n",
    "    \n",
    "    return torch.tensor(pad_input_ids),torch.tensor(label),torch.tensor(pad_attention_mask)\n",
    "    \n",
    "\n",
    "class FamilyForgetDataset(Dataset):\n",
    "    def _init_(self, data_path, tokenizer, model_configs, max_length=512,  unlearn_data_id=0, question_key=None, answer_key=None):\n",
    "        super(FamilyForgetDataset, self)._init_()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = datasets.Dataset.from_dict(torch.load(data_path))\n",
    "        self.data = add_dataset_index(self.data)\n",
    "        self.qk = question_key\n",
    "        self.ak = answer_key\n",
    "        self.unlearn_data_id = unlearn_data_id\n",
    "        self.model_configs = model_configs\n",
    "\n",
    "    def _len_(self):\n",
    "        return int(os.environ.get('WORLD_SIZE', 1)) \n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        pad_input_ids_list = []\n",
    "        label_list = []\n",
    "        pad_attention_mask_list = []\n",
    "        question = self.data[self.unlearn_data_id][self.qk]\n",
    "        answers = self.data[self.unlearn_data_id][self.ak]\n",
    "        indices = self.data[self.unlearn_data_id]['index']\n",
    "        if isinstance(answers, str):\n",
    "            answers = [answers]\n",
    "\n",
    "        pad_input_ids_list = []\n",
    "        label_list = []\n",
    "        pad_attention_mask_list = []\n",
    "\n",
    "        for answer in answers:\n",
    "            converted_data = convert_raw_data_to_model_format(self.tokenizer, self.max_length, question, answer, self.model_configs)\n",
    "            pad_input_ids_list.append(converted_data[0])\n",
    "            label_list.append(converted_data[1])\n",
    "            pad_attention_mask_list.append(converted_data[2])\n",
    "\n",
    "        return torch.stack(pad_input_ids_list).squeeze(),\\\n",
    "                torch.stack(label_list).squeeze(),\\\n",
    "                torch.stack(pad_attention_mask_list).squeeze(),\\\n",
    "                torch.tensor(indices)\n",
    "    \n",
    "def custom_data_collator(samples):\n",
    "    input_ids = [s[0] for s in samples]\n",
    "    labels = [s[1] for s in samples]\n",
    "    attention_mask = [s[2] for s in samples]\n",
    "    return torch.stack(input_ids), torch.stack(labels), torch.stack(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_module import custom_data_collator, FamilyForgetDataset\n",
    "from unlearn_trainer import CustomTrainer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, set_seed\n",
    "\n",
    "import hydra \n",
    "import transformers\n",
    "import os\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "from utils import get_model_identifiers_from_yaml\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "@hydra.main(version_base=None, config_path=\"config\", config_name=\"finetune\")\n",
    "def main(cfg):\n",
    "    num_devices = int(os.environ.get('WORLD_SIZE', 1))\n",
    "    print(f\"num_devices: {num_devices}\")\n",
    "    if os.environ.get('LOCAL_RANK') is not None:\n",
    "        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n",
    "        device_map = {'': local_rank}\n",
    "    set_seed(cfg.seed)\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    model_cfg = get_model_identifiers_from_yaml(cfg.model_family)\n",
    "    model_id = model_cfg[\"model_id\"]\n",
    "\n",
    "    Path(cfg.save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    # save the cfg file\n",
    "    #if master process\n",
    "    if os.environ.get('LOCAL_RANK') is None or local_rank == 0:\n",
    "        with open(f'{cfg.save_dir}/cfg.yaml', 'w') as f:\n",
    "            OmegaConf.save(cfg, f)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    subsample = torch.load(cfg.subsample_path)\n",
    "    shuffled_unlearn_data_id = int(subsample[cfg.unlearn_data_id])\n",
    "    torch_format_dataset = FamilyForgetDataset(cfg.data_path, tokenizer=tokenizer, model_configs=model_cfg,max_length=500, unlearn_data_id=shuffled_unlearn_data_id,question_key='question4', answer_key='answer4') \n",
    "\n",
    "    batch_size = cfg.batch_size\n",
    "    gradient_accumulation_steps = cfg.gradient_accumulation_steps\n",
    "    steps_per_epoch = len(torch_format_dataset)//(batch_size*gradient_accumulation_steps*num_devices)\n",
    "    num_devices = int(os.environ.get('WORLD_SIZE', 1))\n",
    "    print(f\"num_devices: {num_devices}\")\n",
    "\n",
    "    print(\"max_steps calc parmas : len(torch_format_dataset)\", len(torch_format_dataset), \"num_epochs:\", cfg.num_epochs, \"batch_size:\", batch_size, \"gradient_accumulation_steps:\",gradient_accumulation_steps, \"num_devices:\", num_devices, \"steps_per_epoch:\",steps_per_epoch)\n",
    "    max_steps = int(cfg.num_epochs*len(torch_format_dataset))//(batch_size*gradient_accumulation_steps*num_devices)\n",
    "    \n",
    "    lr = float(model_cfg[\"reinforce_lr\"])\n",
    "    \n",
    "    training_args = transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=max(1, max_steps//cfg.num_epochs),\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=lr,\n",
    "            lr_scheduler_type=cfg.lr_scheduler_type,\n",
    "            bf16=True,\n",
    "            bf16_full_eval=True,\n",
    "            logging_steps=max(1,max_steps//20),\n",
    "            logging_dir=f'{cfg.save_dir}/logs',\n",
    "            output_dir=cfg.save_dir,\n",
    "            optim=\"paged_adamw_32bit\",\n",
    "            save_steps=max_steps,\n",
    "            save_strategy=\"steps\",\n",
    "            save_only_model=True,\n",
    "            ddp_find_unused_parameters= False,\n",
    "            evaluation_strategy=\"no\",\n",
    "            deepspeed='config/ds_config.json',\n",
    "            weight_decay = cfg.weight_decay,\n",
    "            seed = cfg.seed,\n",
    "        )\n",
    "\n",
    "    import re\n",
    "    path_found = False\n",
    "    for file in os.listdir(cfg.model_path):\n",
    "        if re.search(r\"pytorch.*\\.bin\", file):\n",
    "            path_found = True\n",
    "            break\n",
    "        \n",
    "        if re.search(r\"model-*\\.safetensors\", file):\n",
    "            path_found = True\n",
    "            break\n",
    "\n",
    "    if path_found:\n",
    "        print(\"INSIDE PATTH FOUND\")\n",
    "        config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "        print(\"Loading from checkpoint\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(cfg.model_path, config=config, use_flash_attention_2=model_cfg[\"flash_attention2\"]==\"true\", torch_dtype=torch.bfloat16, token=os.environ['HF_TOKEN'], trust_remote_code = True)\n",
    "    \n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=model_cfg[\"flash_attention2\"]==\"true\", torch_dtype=torch.bfloat16, trust_remote_code = True)\n",
    "    \n",
    "    # Hot fix for https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035\n",
    "    model.generation_config.do_sample = True\n",
    "\n",
    "    if model_cfg[\"gradient_checkpointing\"] == \"true\":\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        train_dataset=torch_format_dataset,\n",
    "        eval_dataset=torch_format_dataset,\n",
    "        args=training_args,\n",
    "        data_collator=custom_data_collator,\n",
    "    )\n",
    "    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    model.save_pretrained(cfg.save_dir)\n",
    "    tokenizer.save_pretrained(cfg.save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import deepspeed\n",
    "from transformers.integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint, is_deepspeed_available\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids, labels, attention_mask = inputs\n",
    "        # forward pass\n",
    "        outputs = model(input_ids,labels=labels, attention_mask=attention_mask)\n",
    "        # logits = outputs.get(\"logits\")\n",
    "        loss = outputs.loss\n",
    "        # # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        # loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\n",
    "        # loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only: bool, ignore_keys=None):\n",
    "        input_ids, labels, attention_mask = inputs\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids,labels=labels, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "        return (loss, logits, labels)\n",
    "\n",
    "\n",
    "class CustomFamilyTrainerForgetting(Trainer):\n",
    "    def _init_(self, *args, **kwargs):\n",
    "        self.loss_type = kwargs.pop('forget_loss')\n",
    "        self.save_dir = kwargs.pop('save_dir')\n",
    "        self.save_step_pattern = kwargs.pop('save_step_pattern')\n",
    "        super(CustomFamilyTrainerForgetting, self)._init_(*args, **kwargs)\n",
    "        \n",
    "        if self.loss_type == \"npo\":\n",
    "            self.beta = 0.1\n",
    "            self.outputs_f_ref_logits = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        if self.loss_type == \"ga\":\n",
    "            forget_inputs = inputs\n",
    "            input_ids, labels, attention_mask = inputs\n",
    "            outputs = model(input_ids,labels=labels, attention_mask=attention_mask)\n",
    "            forget_loss = outputs.loss\n",
    "            forget_loss = forget_loss * -1\n",
    "            loss = forget_loss\n",
    "            \n",
    "        elif self.loss_type == 'npo':\n",
    "            forget_inputs = inputs\n",
    "            input_ids, labels, attention_mask = forget_inputs\n",
    "            \n",
    "            outputs = model(input_ids,labels=labels, attention_mask=attention_mask)\n",
    "            neg_log_ratio = self.outputs_f_ref_logits - outputs.logits\n",
    "            print(neg_log_ratio)\n",
    "            loss = -F.logsigmoid(self.beta * neg_log_ratio).mean() * 2 / self.beta\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only: bool, ignore_keys=None):\n",
    "        input_ids, labels, attention_mask = inputs\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids,labels=labels, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "        return (loss, logits, labels)\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        eval_dataset = None,\n",
    "        ignore_keys = None,\n",
    "        metric_key_prefix = \"eval\",\n",
    "    ):\n",
    "        curr_step = self.state.global_step\n",
    "        if self.save_step_pattern == \"log\":\n",
    "            import math\n",
    "            if curr_step not in [1, 2, 4, 8, 16, 32]: \n",
    "                return\n",
    "\n",
    "        curr_save_dir = os.path.join(self.save_dir, f\"checkpoint-{curr_step}\")\n",
    "        self.save_model(curr_save_dir)\n",
    "                        \n",
    "    def e_prepare_deepspeed(self, model):\n",
    "        # Adapted from accelerate: https://github.com/huggingface/accelerate/blob/739b135f8367becb67ffaada12fe76e3aa60fefd/src/accelerate/accelerator.py#L1473\n",
    "        deepspeed_plugin = self.accelerator.state.deepspeed_plugin\n",
    "        config_kwargs = copy.deepcopy(deepspeed_plugin.deepspeed_config)\n",
    "\n",
    "        if model is not None:\n",
    "            if hasattr(model, \"config\"):\n",
    "                hidden_size = (\n",
    "                    max(model.config.hidden_sizes)\n",
    "                    if getattr(model.config, \"hidden_sizes\", None)\n",
    "                    else getattr(model.config, \"hidden_size\", None)\n",
    "                )\n",
    "                if hidden_size is not None and config_kwargs[\"zero_optimization\"][\"stage\"] == 3:\n",
    "                    # Note that stage3_prefetch_bucket_size can produce DeepSpeed messages like: Invalidate trace cache @ step 0: expected module 1, but got module 0\n",
    "                    # This is expected and is not an error, see: https://github.com/microsoft/DeepSpeed/discussions/4081\n",
    "                    config_kwargs.update(\n",
    "                        {\n",
    "                            \"zero_optimization.reduce_bucket_size\": hidden_size * hidden_size,\n",
    "                            \"zero_optimization.stage3_param_persistence_threshold\": 10 * hidden_size,\n",
    "                            \"zero_optimization.stage3_prefetch_bucket_size\": 0.9 * hidden_size * hidden_size,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # If ZeRO-3 is used, we shard both the active and reference model.\n",
    "        # Otherwise, we assume the reference model fits in memory and is initialized on each device with ZeRO disabled (stage 0)\n",
    "        if config_kwargs[\"zero_optimization\"][\"stage\"] != 3:\n",
    "            config_kwargs[\"zero_optimization\"][\"stage\"] = 0\n",
    "        config_kwargs[\"optimizer\"] = {\"type\": None}\n",
    "        model, *_ = deepspeed.initialize(model=model, config=config_kwargs)\n",
    "        model.eval()\n",
    "        #set the gradients to false for every parameter\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['down_proj', 'v_proj', 'up_proj', 'gate_proj', 'q_proj', 'o_proj', 'k_proj']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1279787008 || all params: 1279787008 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [retain_train_df, forget_train_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, labels, state_size):\n",
    "    \"\"\"Cross-entropy loss with ignore_index=-100.\"\"\"\n",
    "    return F.cross_entropy(logits.view(-1, state_size), labels.view(-1), ignore_index=-100) \n",
    "\n",
    "\n",
    "def grad_ascent_loss(model, X_f, y_f, state_size):\n",
    "    \"\"\"Gradient ascent loss using cross-entropy.\"\"\"\n",
    "    logits = model.forward(X_f)  # Forward pass to get logits\n",
    "    loss = - cross_entropy_loss(logits, y_f, state_size)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def grad_descent_loss(model, X_r, y_r, state_size):\n",
    "    \"\"\"Gradient descent loss using cross-entropy.\"\"\"\n",
    "    logits = model.forward(X_r)  # Forward pass to get logits\n",
    "    loss = cross_entropy_loss(logits, y_r, state_size)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def grad_diff_loss(model, X_f, y_f, X_r, y_r, state_size):\n",
    "    \"\"\"Gradient difference loss using cross-entropy.\"\"\"\n",
    "    # Forward pass for forget data\n",
    "    logits_f = model.forward(X_f)\n",
    "    loss_f = - cross_entropy_loss(logits_f, y_f, state_size)\n",
    "    \n",
    "\n",
    "    # Forward pass for retain data\n",
    "    logits_r = model.forward(X_r)\n",
    "    loss_r = cross_entropy_loss(logits_r, y_r, state_size)\n",
    "\n",
    "    return loss_f + loss_r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def grad_diff_kl_forget_loss(model, X_f, y_f, X_r, y_r, finetuned_model, state_size):\n",
    "    \"\"\"Gradient difference with KL divergence for forgetting using cross-entropy.\"\"\"\n",
    "\n",
    "    logits_f = model.forward(X_f)\n",
    "    loss_f = -cross_entropy_loss(logits_f, y_f, state_size)  # Cross-entropy with ignore_index=-100\n",
    "\n",
    "\n",
    "    logits_r = model.forward(X_r)\n",
    "    loss_r = cross_entropy_loss(logits_r, y_r, state_size)  # Cross-entropy with ignore_index=-100\n",
    "\n",
    "\n",
    "    # Mask to ignore padding for KL divergence\n",
    "    valid_mask_f = (y_f != -100).float()  # Mask for valid tokens in forget data\n",
    "\n",
    "    # KL divergence with fine-tuned model on forget data, ignoring padding\n",
    "    with torch.no_grad():\n",
    "        logits_f_finetuned = finetuned_model.forward(X_f)\n",
    "\n",
    "\n",
    "    kl_forget = F.kl_div(\n",
    "        F.log_softmax(logits_f, dim=-1) ,  \n",
    "        F.softmax(logits_f_finetuned, dim=-1) ,\n",
    "        reduction='none'\n",
    "    )\n",
    "    kl_forget = kl_forget.sum(dim=-1) * valid_mask_f  # Apply valid mask to KL divergence\n",
    "    # Apply mask to logits before computing KL divergence\n",
    "    kl_forget = kl_forget.sum()/logits_f.size(0)  # Average over the batch\n",
    "\n",
    "    return loss_f + loss_r +  kl_forget\n",
    "\n",
    "\n",
    "def kl_loss(model, X_f, y_f, X_r, y_r, finetuned_model, state_size):\n",
    "    \"\"\"KL divergence-based retain loss using cross-entropy\"\"\"\n",
    "\n",
    "   \n",
    "    logits_f = model.forward(X_f)\n",
    "    loss_f = -cross_entropy_loss(logits_f, y_f, state_size)  # Cross-entropy with ignore_index=-100\n",
    "\n",
    "    # Mask to ignore padding for KL divergence\n",
    "    valid_mask_r = (y_r != -100).float()  # Mask for valid tokens in retain data\n",
    "\n",
    "    # KL divergence with fine-tuned model on retain data, ignoring padding\n",
    "    logits_r = model.forward(X_r)\n",
    "    with torch.no_grad():\n",
    "        logits_r_finetuned = finetuned_model.forward(X_r)\n",
    "\n",
    "    kl_retain = F.kl_div(\n",
    "        F.log_softmax(logits_r, dim=-1) ,\n",
    "        F.softmax(logits_r_finetuned, dim=-1) ,\n",
    "        reduction='none'\n",
    "    )\n",
    "    kl_retain = kl_retain.sum(dim=-1) * valid_mask_r  # Apply valid mask to KL divergence\n",
    "    kl_retain = kl_retain.sum()/logits_r.size(0)  # Average over the batch\n",
    "\n",
    "    return loss_f + kl_retain\n",
    "\n",
    "\n",
    "def NPO_loss(model, X_f, y_f, finetuned_model, beta, state_size):\n",
    "    \"\"\"Non-Paired Objective (NPO) loss using cross-entropy and summing over tokens per sample.\"\"\"\n",
    "\n",
    "    valid_mask_f = (y_f != -100).float()  # Mask for valid tokens, shape [batch_size, seq_len]\n",
    "\n",
    "    # Forward pass for forget data with the current model\n",
    "    logits_f = model.forward(X_f)  # Shape: [batch_size, seq_len, state_size]\n",
    "\n",
    "    y_f_masked = y_f.clone()\n",
    "    y_f_masked[y_f_masked == -100] = 0  # Replace -100 with 0 so gather can function\n",
    "\n",
    "    # Gather the log probabilities corresponding to the true labels\n",
    "    outputs_f = F.log_softmax(logits_f, dim=-1).gather(2, y_f_masked.unsqueeze(-1)).squeeze(-1)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "    # Forward pass for forget data with the fine-tuned model\n",
    "    with torch.no_grad():\n",
    "        logits_f_finetuned = finetuned_model.forward(X_f)\n",
    "        outputs_f_finetuned = F.log_softmax(logits_f_finetuned, dim=-1).gather(2, y_f_masked.unsqueeze(-1)).squeeze(-1)\n",
    "        assert outputs_f_finetuned.shape  == logits_f_finetuned.shape[:2]  # Check shape\n",
    "\n",
    "    # Compute the negative log-ratio, applying the valid mask to ignore padding tokens\n",
    "    neg_log_ratio = (outputs_f_finetuned - outputs_f) * valid_mask_f  # Shape: [batch_size, seq_len]\n",
    "\n",
    "    # Sum over the tokens per sample (along the sequence length dimension)\n",
    "    neg_log_ratio_sum = neg_log_ratio.sum(dim=1)  # Shape: [batch_size]\n",
    "    #import pdb; pdb.set_trace()\n",
    "    # Compute the NPO loss by averaging over the batch, ignoring padding \n",
    "    loss = -F.logsigmoid(beta * neg_log_ratio_sum).mean() * 2 / beta\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def NPO_AVE(model, X_f, y_f, finetuned_model, beta, state_size):\n",
    "\n",
    "#     \"\"\"Non-Paired Objective (NPO) loss using cross-entropy and average over tokens per sample.\"\"\"\n",
    "\n",
    "#     valid_mask_f = (y_f != -100).float()  # Mask for valid tokens, shape [batch_size, seq_len]\n",
    "\n",
    "#     # Forward pass for forget data with the current model\n",
    "#     logits_f = model.forward(X_f)  # Shape: [batch_size, seq_len, state_size]\n",
    "\n",
    "#     y_f_masked = y_f.clone()\n",
    "#     y_f_masked[y_f_masked == -100] = 0  # Replace -100 with 0 so gather can function\n",
    "\n",
    "#     # Gather the log probabilities corresponding to the true labels\n",
    "#     outputs_f = F.log_softmax(logits_f, dim=-1).gather(2, y_f_masked.unsqueeze(-1)).squeeze(-1)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "#     # Forward pass for forget data with the fine-tuned model\n",
    "#     with torch.no_grad():\n",
    "#         logits_f_finetuned = finetuned_model.forward(X_f)\n",
    "#         outputs_f_finetuned = F.log_softmax(logits_f_finetuned, dim=-1).gather(2, y_f_masked.unsqueeze(-1)).squeeze(-1)\n",
    "#         assert outputs_f_finetuned.shape  == logits_f_finetuned.shape[:2]  # Check shape\n",
    "\n",
    "#     # Compute the negative log-ratio, applying the valid mask to ignore padding tokens\n",
    "#     neg_log_ratio = (outputs_f_finetuned - outputs_f) * valid_mask_f  # Shape: [batch_size, seq_len]\n",
    "\n",
    "#     # Average over the tokens per sample (along the sequence length dimension), this is the only line different from NPO_loss\n",
    "#     neg_log_ratio_sum = neg_log_ratio.sum(dim=1)/valid_mask_f.sum(dim=1)  # Shape: [batch_size]\n",
    "#     #import pdb; pdb.set_trace()\n",
    "#     # Compute the NPO loss by averaging over the batch, ignoring padding \n",
    "#     loss = -F.logsigmoid(beta * neg_log_ratio_sum).mean() * 2 / beta\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "\n",
    "# def NPO_NO_REF(model, X_f, y_f, finetuned_model, beta, state_size):\n",
    "\n",
    "#     \"\"\"Non-Paired Objective (NPO) loss using cross-entropy and average over tokens per sample.\"\"\"\n",
    "\n",
    "#     valid_mask_f = (y_f != -100).float()  # Mask for valid tokens, shape [batch_size, seq_len]\n",
    "\n",
    "#     # Forward pass for forget data with the current model\n",
    "#     logits_f = model.forward(X_f)  # Shape: [batch_size, seq_len, state_size]\n",
    "\n",
    "#     y_f_masked = y_f.clone()\n",
    "#     y_f_masked[y_f_masked == -100] = 0  # Replace -100 with 0 so gather can function\n",
    "\n",
    "#     # Gather the log probabilities corresponding to the true labels\n",
    "#     outputs_f = F.log_softmax(logits_f, dim=-1).gather(2, y_f_masked.unsqueeze(-1)).squeeze(-1)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "   \n",
    "\n",
    "#     # Compute the negative log-ratio, applying the valid mask to ignore padding tokens\n",
    "#     neg_log=  - outputs_f * valid_mask_f  # Shape: [batch_size, seq_len]\n",
    "\n",
    "#     # Sum over the tokens per sample (along the sequence length dimension) this is the only line different from SimNPO_loss\n",
    "#     neg_log_sum = neg_log.sum(dim=1)  # Shape: [batch_size]\n",
    "\n",
    "#     #import pdb; pdb.set_trace()\n",
    "#     # Compute the NPO loss by averaging over the batch, ignoring padding \n",
    "#     loss = -F.logsigmoid(beta * neg_log_sum).mean() * 2 / beta\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "\n",
    "def NPO_KL(model, X_f, y_f, X_r, y_r, finetuned_model, beta, state_size):\n",
    "    \"\"\"NPO + KL loss.\"\"\"\n",
    "\n",
    "   \n",
    "    forget_loss = NPO_loss(model, X_f, y_f, finetuned_model, beta, state_size)\n",
    "\n",
    "  \n",
    "    valid_mask_r = (y_r != -100).float()  # Mask for valid tokens in retain data\n",
    "\n",
    "    logits_r = model.forward(X_r)  # [batch_size, seq_len, state_size]\n",
    "    with torch.no_grad():\n",
    "        logits_r_finetuned = finetuned_model.forward(X_r)\n",
    "\n",
    "\n",
    "    retain_probs_current = F.log_softmax(logits_r, dim=-1) \n",
    "    retain_probs_finetuned = F.softmax(logits_r_finetuned, dim=-1)\n",
    "    retain_loss = F.kl_div(retain_probs_current, retain_probs_finetuned, reduction='none')\n",
    "\n",
    "    retain_loss = retain_loss.sum(dim=-1) * valid_mask_r  # Apply mask to KL divergence\n",
    "    retain_loss = retain_loss.sum()/logits_r.size(0)  # Average over the batch\n",
    "\n",
    "\n",
    "    # Final NPO + KL loss\n",
    "    return forget_loss + retain_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def NPO_RT(model, X_f, y_f, X_r, y_r, finetuned_model, beta, state_size):\n",
    "    \"\"\"NPO + Retain loss.\"\"\"\n",
    "\n",
    "\n",
    "    forget_loss = NPO_loss(model, X_f, y_f, finetuned_model, beta, state_size)\n",
    "\n",
    "    \n",
    "    logits_r = model.forward(X_r)  # [batch_size, seq_len, state_size]\n",
    "    retain_loss = cross_entropy_loss(logits_r, y_r, state_size)\n",
    "\n",
    "\n",
    "    # Final NPO + retain loss\n",
    "    return forget_loss + retain_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def SimNPO_loss(model, X_f, y_f, finetuned_model, beta, state_size):\n",
    "    \"\"\"Non-Paired Objective (NPO) loss using cross-entropy and summing over tokens per sample.\"\"\"\n",
    "\n",
    "    valid_mask_f = (y_f != -100).float()  # Mask for valid tokens, shape [batch_size, seq_len]\n",
    "\n",
    "    # Forward pass for forget data with the current model\n",
    "    logits_f = model.forward(X_f)  # Shape: [batch_size, seq_len, state_size]\n",
    "\n",
    "    y_f_masked = y_f.clone()\n",
    "    y_f_masked[y_f_masked == -100] = 0  # Replace -100 with 0 so gather can function\n",
    "\n",
    "    # Gather the log probabilities corresponding to the true labels\n",
    "    outputs_f = F.log_softmax(logits_f, dim=-1).gather(2, y_f_masked.unsqueeze(-1)).squeeze(-1)  # Shape: [batch_size, seq_len]\n",
    "\n",
    "   \n",
    "\n",
    "    # Compute the negative log-ratio, applying the valid mask to ignore padding tokens\n",
    "    neg_log=  - outputs_f * valid_mask_f  # Shape: [batch_size, seq_len]\n",
    "\n",
    "    # Sum over the tokens per sample (along the sequence length dimension)\n",
    "    neg_log_sum = neg_log.sum(dim=1)/valid_mask_f.sum(dim=1)  # Shape: [batch_size]\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "    # Compute the NPO loss by averaging over the batch, ignoring padding \n",
    "    loss = -F.logsigmoid(beta * neg_log_sum).mean() * 2 / beta\n",
    "\n",
    "    return loss\n",
    "\n",
    "def SimNPO_KL(model, X_f, y_f, X_r, y_r, finetuned_model, beta, state_size):\n",
    "    \"\"\"SimNPO + KL loss.\"\"\"\n",
    "\n",
    "   \n",
    "    forget_loss = SimNPO_loss(model, X_f, y_f, finetuned_model, beta, state_size)\n",
    "\n",
    "  \n",
    "    valid_mask_r = (y_r != -100).float()  # Mask for valid tokens in retain data\n",
    "\n",
    "    logits_r = model.forward(X_r)  # [batch_size, seq_len, state_size]\n",
    "    with torch.no_grad():\n",
    "        logits_r_finetuned = finetuned_model.forward(X_r)\n",
    "\n",
    "\n",
    "    retain_probs_current = F.log_softmax(logits_r, dim=-1) \n",
    "    retain_probs_finetuned = F.softmax(logits_r_finetuned, dim=-1)\n",
    "    retain_loss = F.kl_div(retain_probs_current, retain_probs_finetuned, reduction='none')\n",
    "\n",
    "    retain_loss = retain_loss.sum(dim=-1) * valid_mask_r  # Apply mask to KL divergence\n",
    "    retain_loss = retain_loss.sum()/logits_r.size(0)  # Average over the batch\n",
    "\n",
    "\n",
    "    # Final NPO + KL loss\n",
    "    return forget_loss + retain_loss\n",
    "\n",
    "\n",
    "\n",
    "def SimNPO_RT(model, X_f, y_f, X_r, y_r, finetuned_model, beta, state_size):\n",
    "    \"\"\"SimNPO + Retain loss.\"\"\"\n",
    "\n",
    "\n",
    "    forget_loss = SimNPO_loss(model, X_f, y_f, finetuned_model, beta, state_size)\n",
    "\n",
    "    \n",
    "    logits_r = model.forward(X_r)  # [batch_size, seq_len, state_size]\n",
    "    retain_loss = cross_entropy_loss(logits_r, y_r, state_size)\n",
    "\n",
    "\n",
    "    # Final NPO + retain loss\n",
    "    return forget_loss + retain_loss\n",
    "\n",
    "\n",
    "def compute_loss(model, loss_type, X_f, y_f, X_r=None, y_r=None, y_idk=None, finetuned_model=None, state_size=None, beta=None):\n",
    "    \"\"\"\n",
    "    Compute loss based on the specified loss type.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The current model being trained.\n",
    "    - loss_type: The type of loss to compute (e.g., 'grad_ascent', 'grad_diff', 'NPO').\n",
    "    - X_f: Forget data ids.\n",
    "    - y_f: Forget data labels.\n",
    "    - X_r: Retain data ids.\n",
    "    - y_r: Retain data labels.\n",
    "    - y_idk: IDK data labels.\n",
    "\n",
    "    - finetuned_model: The pre-trained model for comparison in KL loss functions.\n",
    "    - state_size: Number of states.\n",
    "    - beta: Scaling factor used in certain loss functions (e.g., non-paired DPO, kto).\n",
    "\n",
    "    Returns:\n",
    "    - Computed loss based on the selected loss_type.\n",
    "    \"\"\"\n",
    "\n",
    "    if loss_type == 'grad_ascent':\n",
    "        return grad_ascent_loss(model, X_f, y_f, state_size)\n",
    "    elif loss_type == 'grad_descent':\n",
    "        return grad_descent_loss(model, X_r, y_r, state_size)\n",
    "    elif loss_type == 'grad_diff':\n",
    "        return grad_diff_loss(model, X_f, y_f, X_r, y_r, state_size)\n",
    "    elif loss_type == 'grad_diff_kl_forget':\n",
    "        return grad_diff_kl_forget_loss(model, X_f, y_f, X_r, y_r, finetuned_model, state_size)\n",
    "    elif loss_type == 'kl':\n",
    "        return kl_loss(model, X_f, y_f, X_r, y_r, finetuned_model, state_size)\n",
    "    elif loss_type == 'NPO':\n",
    "        return NPO_loss(model, X_f, y_f, finetuned_model, beta, state_size)\n",
    "    elif loss_type == 'NPO_KL':\n",
    "        return NPO_KL(model, X_f, y_f, X_r, y_r, finetuned_model, beta, state_size)\n",
    "    elif loss_type == 'NPO_RT':\n",
    "        return NPO_RT(model, X_f, y_f, X_r, y_r, finetuned_model, beta, state_size)\n",
    "    elif loss_type == 'SimNPO':\n",
    "        return SimNPO_loss(model, X_f, y_f, finetuned_model, beta, state_size)\n",
    "    elif loss_type == 'SimNPO_KL':\n",
    "        return SimNPO_KL(model, X_f, y_f, X_r, y_r, finetuned_model, beta, state_size)\n",
    "    elif loss_type == 'SimNPO_RT':\n",
    "        return SimNPO_RT(model, X_f, y_f, X_r, y_r, finetuned_model, beta, state_size)\n",
    "   \n",
    "       \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss type: {loss_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def generate_sequences(transition_matrix, initial_state_probs, num_sequences, seq_length, state_size):\n",
    "    # Convert transition_matrix and initial_state_probs to tensors for efficient sampling\n",
    "    transition_matrix = torch.tensor(transition_matrix, dtype=torch.float32)\n",
    "    initial_state_probs = torch.tensor(initial_state_probs, dtype=torch.float32)\n",
    "\n",
    "    # Initialize sequences array\n",
    "    sequences = torch.zeros((num_sequences, seq_length), dtype=torch.long)\n",
    "\n",
    "    # Sample initial states based on the initial state probabilities\n",
    "    sequences[:, 0] = torch.multinomial(initial_state_probs, num_samples=num_sequences, replacement=True)\n",
    "\n",
    "    for t in range(1, seq_length):\n",
    "        # Get current states\n",
    "        current_states = sequences[:, t - 1]\n",
    "\n",
    "        # Get transition probabilities for the current states\n",
    "        probs = transition_matrix[current_states]  # Shape: [num_sequences, state_size]\n",
    "\n",
    "        # Sample next states for all sequences at once\n",
    "        next_states = torch.multinomial(probs, num_samples=1).squeeze(1)  # Shape: [num_sequences]\n",
    "\n",
    "        # Store the next states\n",
    "        sequences[:, t] = next_states\n",
    "\n",
    "    return sequences.numpy().tolist()\n",
    "\n",
    "def prepare_datasets(state_size=10, seq_lengths=[20,20,20], num_sequences=[10000,5000,5000], seed=42, test_size=0.2,leakage=0.2):\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Create data directory if it doesn't exist\n",
    "\n",
    "    # Define non-intersecting subsets for each distribution\n",
    "    subset_size = state_size // 3\n",
    "    retain_states = list(range(0, subset_size))\n",
    "    forget1_states = list(range(subset_size, 2 * subset_size))\n",
    "    forget2_states = list(range(2 * subset_size, 3 * subset_size))\n",
    "\n",
    "    # Initialize transition matrices with small leakage\n",
    "    #leakage is the probability of transitioning to states outside the subset\n",
    "    retain_transition_matrix = np.full((state_size, state_size), leakage / (state_size - len(retain_states)))\n",
    "    forget_transition_matrix1 = np.full((state_size, state_size), leakage / (state_size - len(forget1_states)))\n",
    "    forget_transition_matrix2 = np.full((state_size, state_size), leakage / (state_size - len(forget2_states)))\n",
    "\n",
    "    # Set higher probabilities for transitions within the subset\n",
    "    for s in retain_states:\n",
    "        retain_transition_matrix[s, retain_states] = (1.0 - leakage) / len(retain_states)\n",
    "    for s in forget1_states:\n",
    "        forget_transition_matrix1[s, forget1_states] = (1.0 - leakage) / len(forget1_states)\n",
    "    for s in forget2_states:\n",
    "        forget_transition_matrix2[s, forget2_states] = (1.0 - leakage) / len(forget2_states)\n",
    "\n",
    "    # Normalize the transition matrices\n",
    "    retain_transition_matrix = retain_transition_matrix / retain_transition_matrix.sum(axis=1, keepdims=True)\n",
    "    forget_transition_matrix1 = forget_transition_matrix1 / forget_transition_matrix1.sum(axis=1, keepdims=True)\n",
    "    forget_transition_matrix2 = forget_transition_matrix2 / forget_transition_matrix2.sum(axis=1, keepdims=True)\n",
    "\n",
    "    # Initial state distributions favoring the subsets\n",
    "    retain_initial_state_probs = np.full(state_size, leakage / (state_size - len(retain_states)))\n",
    "    retain_initial_state_probs[retain_states] = (1.0 - leakage) / len(retain_states)\n",
    "    retain_initial_state_probs = retain_initial_state_probs / retain_initial_state_probs.sum()\n",
    "\n",
    "    forget_initial_state_probs1 = np.full(state_size, leakage / (state_size - len(forget1_states)))\n",
    "    forget_initial_state_probs1[forget1_states] = (1.0 - leakage) / len(forget1_states)\n",
    "    forget_initial_state_probs1 = forget_initial_state_probs1 / forget_initial_state_probs1.sum()\n",
    "\n",
    "    forget_initial_state_probs2 = np.full(state_size, leakage / (state_size - len(forget2_states)))\n",
    "    forget_initial_state_probs2[forget2_states] = (1.0 - leakage) / len(forget2_states)\n",
    "    forget_initial_state_probs2 = forget_initial_state_probs2 / forget_initial_state_probs2.sum()\n",
    "\n",
    "    # Generate sequences\n",
    "    retain_sequences = generate_sequences(\n",
    "        retain_transition_matrix, retain_initial_state_probs,\n",
    "        num_sequences[0], seq_lengths[0], state_size\n",
    "    )\n",
    "    forget_sequences1 = generate_sequences(\n",
    "        forget_transition_matrix1, forget_initial_state_probs1,\n",
    "        num_sequences[1], seq_lengths[1], state_size\n",
    "    )\n",
    "    forget_sequences2 = generate_sequences(\n",
    "        forget_transition_matrix2, forget_initial_state_probs2,\n",
    "        num_sequences[2], seq_lengths[2], state_size\n",
    "    )\n",
    "\n",
    "    # Label sequences with their source transition matrix\n",
    "    retain_sequences_labeled = [('retain', seq) for seq in retain_sequences]\n",
    "    forget_sequences1_labeled = [('forget1', seq) for seq in forget_sequences1]\n",
    "    forget_sequences2_labeled = [('forget2', seq) for seq in forget_sequences2]\n",
    "\n",
    "    # Split retain, forget1, and forget2 sequences into train and test sets\n",
    "    retain_train_seqs, retain_test_seqs = train_test_split(retain_sequences_labeled, test_size=test_size, random_state=seed)\n",
    "    forget1_train_seqs, forget1_test_seqs = train_test_split(forget_sequences1_labeled, test_size=test_size, random_state=seed)\n",
    "    forget2_train_seqs, forget2_test_seqs = train_test_split(forget_sequences2_labeled, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # Combine forget1 and forget2 sequences\n",
    "    forget_train_seqs = forget1_train_seqs + forget2_train_seqs\n",
    "    forget_test_seqs = forget1_test_seqs + forget2_test_seqs\n",
    "\n",
    "    # Shuffle forget sequences after combining\n",
    "    random.shuffle(forget_train_seqs)\n",
    "    random.shuffle(forget_test_seqs)\n",
    "\n",
    "    # Combine retain and forget sequences\n",
    "    all_train_sequences = retain_train_seqs + forget_train_seqs\n",
    "    all_test_sequences = retain_test_seqs + forget_test_seqs\n",
    "\n",
    "    # Shuffle all combined sequences\n",
    "    random.shuffle(all_train_sequences)\n",
    "    random.shuffle(all_test_sequences)\n",
    "\n",
    "    # Find the maximum sequence length for padding\n",
    "    max_seq_length = max(seq_lengths)\n",
    "\n",
    "    # Save datasets to files\n",
    "    data = {\n",
    "        'retain_train_sequences': retain_train_seqs,\n",
    "        'retain_test_sequences': retain_test_seqs,\n",
    "        'forget1_train_sequences': forget1_train_seqs,\n",
    "        'forget1_test_sequences': forget1_test_seqs,\n",
    "        'forget2_train_sequences': forget2_train_seqs,\n",
    "        'forget2_test_sequences': forget2_test_seqs,\n",
    "        'forget_train_sequences': forget_train_seqs,\n",
    "        'forget_test_sequences': forget_test_seqs,\n",
    "        'all_train_sequences': all_train_sequences,\n",
    "        'all_test_sequences': all_test_sequences,\n",
    "        'max_seq_length': max_seq_length,\n",
    "        'state_size': state_size,\n",
    "        'seq_lengths': seq_lengths,\n",
    "        'num_sequences': num_sequences,\n",
    "        'seed': seed,\n",
    "        'retain_initial_state_probs': retain_initial_state_probs,\n",
    "        'forget_initial_state_probs1': forget_initial_state_probs1,\n",
    "        'forget_initial_state_probs2': forget_initial_state_probs2,\n",
    "        'retain_transition_matrix': retain_transition_matrix,\n",
    "        'forget_transition_matrix1': forget_transition_matrix1,\n",
    "        'forget_transition_matrix2': forget_transition_matrix2,\n",
    "        'leakage': leakage,\n",
    "    }\n",
    "\n",
    "    if 1:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ],\n",
       "       [0.02857143, 0.02857143, 0.02857143, 0.02857143, 0.02857143,\n",
       "        0.02857143, 0.26666667, 0.26666667, 0.26666667, 0.02857143],\n",
       "       [0.02857143, 0.02857143, 0.02857143, 0.02857143, 0.02857143,\n",
       "        0.02857143, 0.26666667, 0.26666667, 0.26666667, 0.02857143],\n",
       "       [0.02857143, 0.02857143, 0.02857143, 0.02857143, 0.02857143,\n",
       "        0.02857143, 0.26666667, 0.26666667, 0.26666667, 0.02857143],\n",
       "       [0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "        0.1       , 0.1       , 0.1       , 0.1       , 0.1       ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['forget_transition_matrix2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrices = {\n",
    "        'retain': data['retain_transition_matrix'],\n",
    "        'forget1': data['forget_transition_matrix1'],\n",
    "        'forget2': data['forget_transition_matrix2'],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Custom Dataset class with padding\n",
    "class OneHotDataset(Dataset):\n",
    "    def __init__(self, sequences_labeled, max_seq_length):\n",
    "        self.sequences_labeled = sequences_labeled\n",
    "        self.max_seq_length = max_seq_length - 1  # Adjust for input_ids and labels length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences_labeled)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, seq = self.sequences_labeled[idx]\n",
    "        input_ids = seq[:-1]  # All except last token\n",
    "        labels = seq[1:]      # All except first token\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_seq_length - len(input_ids)\n",
    "        input_ids_padded = input_ids + [0] * padding_length\n",
    "        labels_padded = labels + [-100] * padding_length  # Use -100 to ignore padding in loss\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids_padded, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels_padded, dtype=torch.long),\n",
    "            'label': label\n",
    "        }\n",
    "max_seq_length = 512\n",
    "retain_train_dataset = OneHotDataset(retain_train_df, max_seq_length)\n",
    "forget_train_dataset = OneHotDataset(forget_train_df, max_seq_length)\n",
    " \n",
    "retain_test_dataset = OneHotDataset(retain_validation_df, max_seq_length)\n",
    "forget_test_dataset = OneHotDataset(forget_validation_df, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fab469bbfd0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_train_dataloader = DataLoader(forget_train_dataset, batch_size=8, shuffle=True)\n",
    "forget_train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.OneHotDataset at 0x7fa98f5ce5f0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrain_train_dataloader = DataLoader(retain_train_dataset, batch_size=8, shuffle=True)\n",
    "retain_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'6adbf83c-5071-4979-bedb-e5184b15650bsc1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tofu/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '6adbf83c-5071-4979-bedb-e5184b15650bsc1'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretain_train_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m6adbf83c-5071-4979-bedb-e5184b15650bsc1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mOneHotDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 19\u001b[0m     label, seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences_labeled\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m seq[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# All except last token\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     labels \u001b[38;5;241m=\u001b[39m seq[\u001b[38;5;241m1\u001b[39m:]      \u001b[38;5;66;03m# All except first token\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tofu/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/tofu/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: '6adbf83c-5071-4979-bedb-e5184b15650bsc1'"
     ]
    }
   ],
   "source": [
    "retain_train_dataset.__getitem__(\"6adbf83c-5071-4979-bedb-e5184b15650bsc1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1052",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tofu/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1052",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m forget_batch, retain_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(forget_train_dataloader, retrain_train_dataloader):\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m             \u001b[38;5;66;03m# Prepare forget batch data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m             input_ids_f \u001b[38;5;241m=\u001b[39m forget_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m             labels_f \u001b[38;5;241m=\u001b[39m forget_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/tofu/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mOneHotDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 19\u001b[0m     label, seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequences_labeled\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     20\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m seq[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# All except last token\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     labels \u001b[38;5;241m=\u001b[39m seq[\u001b[38;5;241m1\u001b[39m:]      \u001b[38;5;66;03m# All except first token\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tofu/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/tofu/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1052"
     ]
    }
   ],
   "source": [
    "for forget_batch, retain_batch in zip(forget_train_dataloader, retrain_train_dataloader):\n",
    "\n",
    "\n",
    "            # Prepare forget batch data\n",
    "            input_ids_f = forget_batch['input_ids']\n",
    "            labels_f = forget_batch['labels']\n",
    "            \n",
    "            # Prepare retain batch data\n",
    "            input_ids_r = retain_batch['input_ids']\n",
    "            labels_r = retain_batch['labels']\n",
    "\n",
    "            print(labels_f,labels_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size=10\n",
    "seq_length_retain=20\n",
    "seq_length_forget1=20\n",
    "seq_length_forget2=20\n",
    "num_retain_sequences=10000\n",
    "num_forget_sequences1=5000\n",
    "num_forget_sequences2=5000\n",
    "data_dir=\"data\"\n",
    "data_seed=42\n",
    "training_seed=42\n",
    "unlearning_seed=42\n",
    "test_size=0.2\n",
    "leakage=0.2\n",
    "\n",
    "\n",
    "learning_rate = 5e-4\n",
    "batch_size =8\n",
    "epoch = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of embedding layers: 1\n"
     ]
    }
   ],
   "source": [
    "embedding_count = sum(1 for layer in model.modules() if isinstance(layer, nn.Embedding))\n",
    "print(f\"Total number of embedding layers: {embedding_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available()  else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type ='NPO_KL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = model.to(device)\n",
    "intial_model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_model(model, dataloaders, criterion, state_size, device, retrain_model=None):\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    if retrain_model:\n",
    "        retrain_model.eval()  # Put retrain model in eval mode if provided\n",
    "    \n",
    "    results = {}\n",
    "    with torch.no_grad():\n",
    "        for name, dataloader in dataloaders.items():\n",
    "\n",
    "            \n",
    "            \n",
    "            total_loss = 0\n",
    "            total_tokens = 0\n",
    "            \n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                # Forward pass through the main model\n",
    "                logits = model(input_ids)  # [batch_size, seq_length, state_size]\n",
    "\n",
    "               \n",
    "                if retrain_model:\n",
    "                    # Forward pass through the retrain model\n",
    "                    retrain_logits = retrain_model(input_ids)  # [batch_size, seq_length, state_size]\n",
    "                    \n",
    "                    # Compute log probabilities of the current model and retrain model\n",
    "                    log_probs = F.log_softmax(logits, dim=-1)\n",
    "                    retrain_probs = F.softmax(retrain_logits, dim=-1)\n",
    "\n",
    "                    # Mask to identify non-padded tokens\n",
    "                    non_pad_mask = (labels != -100)\n",
    "                    \n",
    "                    # Compute KL divergence only on non-padded tokens\n",
    "                    kl_div = F.kl_div(log_probs, retrain_probs, reduction='none').sum(dim=-1)  # Sum over state_size\n",
    "                    non_pad_tokens = non_pad_mask.sum().item()\n",
    "\n",
    "                    total_loss += kl_div[non_pad_mask].sum().item()  # Sum KL divergence for non-padded tokens\n",
    "                else:\n",
    "                    # Standard cross-entropy loss for non-padded tokens\n",
    "                    loss = criterion(logits.view(-1, state_size), labels.view(-1))\n",
    "                    non_pad_tokens = (labels != -100).sum().item()\n",
    "                    total_loss += loss.item() * non_pad_tokens\n",
    "\n",
    "                total_tokens += non_pad_tokens\n",
    "\n",
    "            avg_loss = total_loss / total_tokens\n",
    "            results[name] = avg_loss\n",
    "            \n",
    "    return results\n",
    "\n",
    "def evaluate_with_transitions(model, dataloaders, transition_matrices, state_size, device, retrain_model=None):\n",
    "    \n",
    "    model.eval()\n",
    "    if retrain_model:\n",
    "        retrain_model.eval()  # Set the retrain model in evaluation mode if provided\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, dataloader in dataloaders.items():\n",
    "\n",
    "            total_kl_div = 0\n",
    "            total_tokens = 0\n",
    "            \n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)  # [batch_size, seq_length]\n",
    "                labels = batch['labels'].to(device)  # [batch_size, seq_length]\n",
    "                labels_seq = batch['label']  # List of sequence labels indicating retain, forget1, or forget2\n",
    "\n",
    "                logits = model(input_ids)  # [batch_size, seq_length, state_size]\n",
    "                \n",
    "                \n",
    "                if retrain_model:\n",
    "                    # Compute logits for the retrain model\n",
    "                    retrain_logits = retrain_model(input_ids)  # [batch_size, seq_length, state_size]\n",
    "                    retrain_probs = torch.softmax(retrain_logits, dim=-1)  # Softmax over logits of the retrain model\n",
    "                else:\n",
    "                    # Compute transition probabilities for each sequence from the provided transition matrices\n",
    "                    batch_size, seq_length = input_ids.shape\n",
    "                    transition_probs = np.zeros((batch_size, seq_length, state_size))\n",
    "\n",
    "                    label_to_matrix = {\n",
    "                        'retain': transition_matrices['retain'],\n",
    "                        'forget1': transition_matrices['forget1'],\n",
    "                        'forget2': transition_matrices['forget2']\n",
    "                    }\n",
    "\n",
    "                    for i, seq_label in enumerate(labels_seq):\n",
    "                        trans_matrix = label_to_matrix.get(seq_label)\n",
    "                        if trans_matrix is None:\n",
    "                            continue\n",
    "                        current_states = input_ids[i].cpu().numpy()\n",
    "                        transition_probs[i] = trans_matrix[current_states]\n",
    "\n",
    "                    # Convert transition probabilities to torch tensor\n",
    "                    transition_probs = torch.tensor(transition_probs, dtype=torch.float32, device=device)  # [batch_size, seq_length, state_size]\n",
    "\n",
    "                # Get the mask for non-padding tokens\n",
    "                non_pad_mask = (labels != -100)  # [batch_size, seq_length]\n",
    "\n",
    "                # Compute log probabilities for the main model\n",
    "                log_probs = F.log_softmax(logits, dim=-1)  # [batch_size, seq_length, state_size]\n",
    "                \n",
    "                if retrain_model:\n",
    "                    # Compute KL divergence between main model and retrain model probabilities\n",
    "                    kl_div = F.kl_div(log_probs, retrain_probs, reduction='none').sum(-1)  # [batch_size, seq_length]\n",
    "                else:\n",
    "                    # Compute KL divergence between main model and transition probabilities\n",
    "                    kl_div = F.kl_div(log_probs, transition_probs, reduction='none').sum(-1)  # [batch_size, seq_length]\n",
    "\n",
    "                # Mask out padding tokens\n",
    "                kl_div = kl_div * non_pad_mask.float()\n",
    "\n",
    "                total_kl_div += kl_div.sum().item()\n",
    "                total_tokens += non_pad_mask.sum().item()\n",
    "\n",
    "            avg_kl_div = total_kl_div / total_tokens   # Average KL divergence over non-padded tokens\n",
    "            results[name] = avg_kl_div\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr= learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tofu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
